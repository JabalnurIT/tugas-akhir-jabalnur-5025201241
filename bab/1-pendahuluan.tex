\chapter{PENDAHULUAN}\label{chap:pendahuluan}

% Ubah bagian-bagian berikut dengan isi dari pendahuluan

\section{Latar Belakang}\label{sec:latarbelakang}

Analisis data modern memerlukan penanganan data berskala 
terabyte yang tersebar di berbagai mesin. Untuk membuat 
ini dapat dilakukan, kerangka kerja 
\emph{Data Intensive Scalable Computing (DISC)} seperti 
Apache Spark~(\cite{zaharia2010,spark}) dan 
Google MapReduce~(\cite{dean2008}) memungkinkan analis 
data untuk membuat aplikasi terdistribusi. Volume besar 
data input untuk aplikasi DISC, ditambah dengan sifat 
terdistribusi dari program, membuat debugging menjadi 
sangat menantang.
Bayangkan sebuah program yang menghitung rata-rata 
curah hujan per tahun di Amerika Serikat. Ini 
memerlukan penghitungan rata-rata dari puluhan 
juta nilai yang dikumpulkan dari sensor yang 
tersebar di seluruh negeri. Setelah menjalankan 
program, analis memperhatikan bahwa nilai rata-rata 
yang dihasilkan sangat tinggi dan mencurigakan. 
Bagaimana programmer akan mendiagnosis penyebab 
kesalahan ini?

Beberapa penelitian dalam \emph{debugging} program telah berfokus pada pengurangan ukuran \emph{input} yang menyebabkan kesalahan. Zeller, misalnya, mengembangkan teknik yang mirip dengan \emph{delta debugging} untuk menyederhanakan kasus uji yang gagal, seperti prototipe yang menyederhanakan 896 baris HTML menjadi satu baris yang menyebabkan kegagalan~(\cite{zeller2002}). Misherghi memperkenalkan HDD, sebuah algoritma yang mempercepat \emph{delta debugging} dengan menerapkan teknik ini pada struktur data input, bertujuan memangkas bagian besar dari input yang tidak relevan~(\cite{misherghi2006}). Kirschner menghadirkan ddmax, yang secara otomatis memperbaiki kesalahan pada data dengan mencari perbedaan antara input yang gagal dan input "maksimal" yang berhasil, mencakup semua fragmen input yang tidak dapat diproses~(\cite{kirschner2020}). Clause, di sisi lain, menggunakan teknik dynamic tainting dengan menandai input saat memasuki aplikasi dan melacak penyebarannya untuk mengidentifikasi input yang relevan~(\cite{clause2009}). Teknik-teknik tersebut adalah varian dari algoritma \emph{delta debugging}, yang bekerja dengan menjalankan program berulang kali dengan segmen-segmen berbeda dari input hingga menemukan ukuran input minimal. \emph{Delta debugging} adalah teknik yang digunakan dalam pemrograman untuk menyederhanakan dan memperkecil ukuran input atau kasus uji yang menyebabkan kesalahan dalam program. Tujuannya adalah untuk menemukan bagian terkecil dari input yang masih menyebabkan kesalahan, sehingga memudahkan pengidentifikasian dan perbaikan masalah. Teknik pengurangan input tradisional ini, meskipun telah dicoba~(\cite{gulzar2018}), tidak dapat diterapkan secara efektif pada aplikasi DISC karena memerlukan eksekusi berulang yang lambat dan memakan banyak sumber daya.

Pada penelitian yang lain, Gulzar mengusulkan teknik yang disebut 
BigTest, sebuah kerangka pengujian \emph{white-box} yang dirancang untuk 
secara otomatis menghasilkan data sintetik konkret guna meningkatkan 
efektivitas dan efisiensi pengujian aplikasi Apache Spark. 
Kelebihan dari BigTest termasuk kemampuannya untuk menghasilkan 
data uji yang dapat mengungkapkan hingga 2 kali lebih banyak cacat 
dibandingkan dengan seluruh dataset dan mengurangi waktu pengujian 
hingga 194 kali lebih cepat. BigTest juga mengintegrasikan eksekusi 
simbolik dari fungsi yang ditentukan pengguna (UDF) dengan 
spesifikasi logis dari operator aliran data dan relasional 
untuk mengeksplorasi semua jalur dalam aplikasi DISC. 
Walaupun BigTest dapat digunakan pada aplikasi DISC, 
kelemahan dari teknik ini yaitu potensi masalah dalam 
menangani fungsi UDF yang sangat kompleks. Selain itu, meskipun 
BigTest mengklaim efisiensi waktu pengujian, skalabilitas untuk 
dataset yang sangat besar dan kompleks tetap menjadi tantangan, 
dan data sintetik yang dihasilkan mungkin tidak sepenuhnya 
mencerminkan kondisi dunia nyata yang dikarenakan oleh
penggunaan fungsi UDF yang sangat kompleks~(\cite{gulzar2020}).


Dalam penelitian ini, kami mengusulkan teknik berbasis 
\emph{large language model} (LLM) yang efisien untuk 
merangkum data yang bermasalah, yang mungkin mencakup 
jutaan baris, menjadi hanya beberapa baris yang tetap 
dapat mereproduksi kesalahan. Kami mewujudkan ide ini 
dalam alat yang disebut FISUM, sebuah sistem 
\emph{Fault-inducing Inputs Summarization}. 
Inti dari teknik kami adalah bahwa LLM generatif 
modern dapat dilatih untuk menangkap pola input 
yang memicu kesalahan dari data besar. Hasil dari 
pelatihan model ini kemudian dapat digunakan untuk 
menghasilkan input baru yang menyebabkan pola 
kesalahan yang sama.

FISUM memerlukan proses \emph{pre-training} 
dari model GPT-2 versi ringan, distilGPT2, pada seluruh 
data input. Langkah \emph{unsupervised} ini 
memungkinkan model untuk mempelajari struktur 
dasar dan pola dari data tersebut. FISUM kemudian 
menggunakan Titian untuk mengambil subset besar 
dari input yang dianggap mencurigakan terkait 
dengan eksekusi yang salah. Titian merupakan sebuah 
\textit{Data Provenance Engine} 
yang dibangun untuk Apache Spark dengan tujuan untuk memulihkan input yang 
menyebabkan kesalahan dengan cara melacak data\emph{input} mana 
saja yang mempengaruhi hasil mencurigakan 
pada aplikasi tersebut~(\cite{interlandi2015}).
DistilGPT2 kemudian 
dilatih ulang pada data mencurigakan ini untuk 
membiasakannya dalam menghasilkan input yang 
menyebabkan kesalahan. DistilGPT2 tersebut dapat diakses menggunakan
sebuah API yang dikembangkan menggunakan FastAPI. 
FastAPI adalah sebuah kerangka kerja web modern yang
dikembangkan menggunakan bahasa pemrograman Python
yang memungkinkan pengembang untuk membuat API
dengan cepat dan mudah~(\cite{fastapi}). Data yang dikirim ke API
harus berupa data \emph{unstructure text} sehingga
\emph{faulty input} yang dihasilkan oleh Titian akan diubah terlebih dahulu
dengan cara menggabungkan kolom data menggunakan "," sebagai
pemisah antar kolom dan menggabungkan baris data menggunakan
"\#\#\#" sebagai pemisah antar baris.

DistilGPT2 yang telah selesai dilatih dapat digunakan
untuk menghasilkan \emph{faulty input} baru yang
memiliki kemiripan karakteristik dengan \emph{faulty input}
yang telah diidentifikasi hanya saja dengan ukuran
yang lebih kecil. \emph{Faulty input} baru ini kemudian dapat
digunakan oleh pengembang untuk melakukan analisis
lebih lanjut terhadap kesalahan yang terjadi pada \emph{input} aplikasi
DISC. Untuk menguji hasil dari FISUM, \emph{faulty input} 
baru yang dihasilkan dapat dijalankan ulang pada aplikasi DISC
dengan tetap menanamkan Titian pada aplikasi tersebut untuk
mengidentifikasi apakah \emph{faulty input} baru tersebut
dapat menyebabkan kesalahan yang sama dengan \emph{faulty input}
yang telah diidentifikasi sebelumnya. Penelitian ini
akan menggunakan 16 \emph{benchmark program} yang berjalan
pada aplikasi DISC untuk menguji keefektifan dari FISUM
yang berasal dari penelitian 
sebelumnya yaitu \emph{age analysis, commute type, commute type full,
customers, delays}~(\cite{gulzar2019})\emph{, delivery faults, external call,
find salary, flight distance, income aggregation,
inside circle, loan type, movie rating}~(\cite{humayun2023})\emph{, 
number series, student grade, dan
word count}~(\cite{zhang2021}).
\\
\\
\\


\section{Rumusan Permasalahan}
\label{sec:permasalahan}

Berdasarkan latar belakang di atas, maka dapat ditarik rumusan masalah sebagai berikut:

\begin{enumerate}[nolistsep]

   \item Bagaimana cara mengidentifikasi \emph{faulty input} dari aplikasi DISC?

   \item Bagaimana cara memproduksi \emph{faulty input} baru dari \emph{faulty input} yang telah teridentifikasi dalam waktu yang singkat?

   \item Bagaimana cara mengevaluasi kemiripan karakteristik antara \emph{faulty input} baru dengan \emph{faulty input} yang telah teridentifikasi?

\end{enumerate}

\section{Batasan Masalah}
\label{sec:batasanmasalah}

Batasan masalah penelitian ini adalah sebagai berikut:

\begin{enumerate}[nolistsep]

  \item Sumber dana dari penelitian ini berasal dari dana LPDP pada program penelitian \emph{Garuda ACE} tahun 2022-2024 yang diselenggarakan oleh Direktorat Jenderal Pendidikan Tinggi,
  Kementerian Pendidikan, Kebudayaan, Riset, dan Teknologi
  
  \item Implementasi algoritma menggunakan bahasa Python dan Scala.

  \item Pembuatan sistem hanya berfokus pada proses \emph{debugging} dan \emph{generate new faulty input} bukan pada aplikasi DISC yang test inputnya akan didebug.

  \item Proses \emph{debugging} dan \emph{generate new faulty input} hanya berfokus pada 16 \emph{benchmark program} yang berjalan di aplikasi DISC. 

  \item Test input berupa \emph{structure text}.

\end{enumerate}

\section{Tujuan}
\label{sec:Tujuan}

Tujuan penelitian ini adalah sebagai berikut:

\begin{enumerate}[nolistsep]

  \item Membuat sebuah sistem yang dapat mengidentifikasi \emph{faulty input} yang menyebabkan kesalahan dalam aplikasi DISC.
  \item Membuat sebuah sistem yang dapat memproduksi \emph{faulty input} baru berdasarkan \emph{faulty input} yang telah diidentifkasi dalam waktu yang singkat.
  \item Membuat sebuah sistem yang dapat memproduksi \emph{faulty input} baru yang memiliki kemiripan karakteristik dengan \emph{faulty input} yang telah diidentifikasi.

\end{enumerate}

\section{Manfaat}
\label{sec:Manfaat}

Manfaat penelitian ini adalah sebagai berikut:

\begin{enumerate}[nolistsep]
  \item Meningkatkan kualitas perangkat lunak di sistem \emph{Data-Intensive Scalable Computing (DISC)} dengan memastikan pemrosesan data yang lebih efisien dan mengurangi kesalahan.
  
  \item Menghemat waktu dan usaha dalam mengidentifikasi dan memperbaiki masalah aplikasi DISC melalui metode yang efisien untuk deteksi dan analisis kesalahan.
  
  \item Meningkatkan produktivitas pengembang perangkat lunak dengan menyediakan alat otomatisasi untuk menghasilkan dan menguji \emph{faulty input} dengan lebih cepat.
  
  \item Mendukung penelitian aplikasi DISC dengan menawarkan metodologi dan alat untuk mengevaluasi performa sistem dalam menangani data besar dan meningkatkan efektivitas aplikasi.
  \end{enumerate}
